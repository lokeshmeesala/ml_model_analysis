{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6070ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression, fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fc3e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Data\n",
    "# housing = fetch_california_housing()\n",
    "# print(housing.data.shape, housing.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb78847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(ytrue: np.array, ypred: np.array):\n",
    "    error = y-ypred\n",
    "    mae = np.sum(abs(error))/y.shape[0]\n",
    "    mse = np.sum((error)**2)/y.shape[0]\n",
    "    rmse = mse**0.5\n",
    "    return {\"MSE\": mse, \"MAE\": mae, \"RMSE\": rmse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8562d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Formula\n",
    "# Normal Equation: theta = (X^T * X)^-1 * X^T * y => here X is the input ndarray (matrix) and y is the target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08803b28",
   "metadata": {},
   "source": [
    "## Normal Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bee5e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinReg_Norm:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.theta = None\n",
    "        \n",
    "    def fit(self):\n",
    "        # normal equation: theta = (X^T * X)^-1 * X^T * y\n",
    "        \n",
    "        # We need to append a row on 1's as an itercept to the input X array.\n",
    "        X_1 = np.c_[np.ones((self.X.shape[0], 1)), self.X]\n",
    "        X_1T = X_1.T\n",
    "        self.theta = np.linalg.inv(X_1T.dot(X_1)).dot(X_1T).dot(y)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # We need to append a row on 1's as an itercept to the input X_test array.\n",
    "        X_test1 = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        y_pred = X_test1.dot(self.theta)\n",
    "        return y_pred\n",
    "        \n",
    "    def fit_predict(self):\n",
    "        self.fit()\n",
    "        return self.predict(self.X)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea10cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=100, n_features=10, noise=5, random_state=42)\n",
    "\n",
    "lr_norm = LinReg_Norm(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb90e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_norm.fit()\n",
    "ypred = lr_norm.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7582b926",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(y, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14a8081",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ypred, marker='x', color='red', linestyle='None', label='y_pred')\n",
    "plt.plot(y, marker='.', color='blue', linestyle='None', label='y_true')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbf6758",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23ab531",
   "metadata": {},
   "source": [
    "$$ \\text{MSE Loss Function: } J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)} \\right)^2 $$\n",
    "    \n",
    "\n",
    "$$ \\text{Loss Function Partial Derivative: } \\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)} \\right) x_j^{(i)} $$\n",
    "\n",
    "$$ \\text{The hypothesis function can be written in matrix form as } h_{\\theta}(X)=X{\\theta}$$\n",
    "\n",
    "$$ \\text{Vectorized Gradient Calculation: } \\nabla_{\\theta} J(\\theta) = \\frac{1}{m} X^T (X\\theta - y) $$\n",
    "\n",
    "$$ \\text{Gradient} = \\frac{1}{m} X^T (X\\theta - y) $$\n",
    "\n",
    "$$ \\text{Gradient Update: } \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487a18f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_gradient_descent(X, y, learning_rate, num_iterations, batch_size):\n",
    "    num_samples, num_features = X.shape\n",
    "    theta = np.zeros((num_features, 1))\n",
    "    cost_history = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        total_cost = 0.0\n",
    "        \n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        \n",
    "        for j in range(0,num_samples,batch_size):\n",
    "            x_batch = X_shuffled[j:batch_size+j]\n",
    "            y_batch = y_shuffled[j:batch_size+j]\n",
    "            \n",
    "            # Compute prediction and error for the single batch\n",
    "            prediction = np.dot(x_batch, theta)\n",
    "            error = prediction - y_batch\n",
    "\n",
    "            # Update parameters using the gradient for the single batch\n",
    "            gradient = (1 / batch_size) * np.dot(x_batch.T, error)\n",
    "            theta -= learning_rate * gradient\n",
    "            \n",
    "            cost_batch = (1 / (2 * batch_size)) * np.sum(np.square(error))\n",
    "\n",
    "            total_cost += cost_batch\n",
    "        \n",
    "        avg_cost = total_cost / (num_samples/batch_size)\n",
    "        cost_history.append(avg_cost)\n",
    "    \n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a609b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinReg_GradDescent:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.y = y.reshape(-1, 1)\n",
    "        self.theta = None\n",
    "        \n",
    "    def fit(self, learning_rate, num_iterations, batch_size):\n",
    "        num_samples, num_features = self.X.shape\n",
    "        theta = np.zeros((num_features, 1))\n",
    "        theta\n",
    "        cost_history = []\n",
    "\n",
    "        for i in range(num_iterations):\n",
    "            total_cost = 0.0\n",
    "\n",
    "            indices = np.arange(num_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X_shuffled = self.X[indices]\n",
    "            y_shuffled = self.y[indices]\n",
    "\n",
    "            for j in range(0,num_samples,batch_size):\n",
    "                x_batch = X_shuffled[j:batch_size+j]\n",
    "                y_batch = y_shuffled[j:batch_size+j]\n",
    "\n",
    "                prediction = np.dot(x_batch, theta)\n",
    "                error = prediction - y_batch\n",
    "\n",
    "                gradient = (1 / batch_size) * np.dot(x_batch.T, error)\n",
    "                theta -= learning_rate * gradient\n",
    "\n",
    "\n",
    "                cost_batch = (1 / (2 * batch_size)) * np.sum(np.square(error))\n",
    "\n",
    "                total_cost += cost_batch\n",
    "\n",
    "            avg_cost = total_cost / (num_samples/batch_size)\n",
    "            cost_history.append(avg_cost)\n",
    "        \n",
    "        self.theta = theta\n",
    "        return cost_history\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # We need to append a row on 1's as an itercept to the input X_test array.\n",
    "        X_with_intercept = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "        y_pred = np.dot(X_test_with_intercept, self.theta)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4710a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_grad = LinReg_GradDescent(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5463ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_history_1 = lr_grad.fit(learning_rate=0.01, num_iterations=1000, batch_size=1)\n",
    "cost_history_25 = lr_grad.fit(learning_rate=0.01, num_iterations=1000, batch_size=25)\n",
    "cost_history_100 = lr_grad.fit(learning_rate=0.01, num_iterations=1000, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243dc015",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(cost_history_g, color='red', linestyle='-', label='y_pred')\n",
    "plt.plot(cost_history_1, color='blue', linestyle='-', label='y_pred')\n",
    "plt.plot(cost_history_25, color='green', linestyle='-', label='y_pred')\n",
    "plt.plot(cost_history_100, color='yellow', linestyle='-.', label='y_pred')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db135b8",
   "metadata": {},
   "source": [
    "## Assumptions in Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d632c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Linearity - Check scatter plots of features vs. target\n",
    "for i in range(X.shape[1]):\n",
    "    plt.scatter(X[:,i], y)\n",
    "    plt.xlabel(f'Feature_{i}')\n",
    "    plt.ylabel('Target')\n",
    "    plt.title(f'Feature_{i} vs. Target')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f456160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Independence - Check residuals vs. predicted values\n",
    "y_pred = lr_grad.predict(X).reshape(-1)\n",
    "residuals = y - y_pred\n",
    "plt.scatter(y_pred, residuals)\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs. Predicted Values')\n",
    "plt.axhline(y=0, color='red', linestyle='--')  # Add a horizontal line at y=0 for reference\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcd622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Homoscedasticity - Check residuals vs. predicted values for constant spread\n",
    "plt.scatter(y_pred, np.abs(residuals))\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Absolute Residuals')\n",
    "plt.title('Absolute Residuals vs. Predicted Values')\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0650d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Normality of Residuals - Check histogram and Q-Q plot of residuals\n",
    "plt.hist(residuals, bins=20)\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467464a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fac0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. No Multicollinearity - Check correlation matrix and VIF values\n",
    "# correlation_matrix = X.corr()\n",
    "correlation_matrix = pd.DataFrame(X).corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2272db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "print('VIF values:')\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ca353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
