{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f53c360",
   "metadata": {},
   "source": [
    "# Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f638ab",
   "metadata": {},
   "source": [
    "To decide which column is the best for the current node we have 3 popular metrics;\n",
    "\n",
    "    Gini Impurity.\n",
    "    Information Gain.\n",
    "    Variance reduction.\n",
    "    \n",
    "### Background\n",
    "#### Entropy: \n",
    "    Entropy is a measure of the uncertainty or randomness of a system (In this case a feature, outcome of selecting a threshold for a feature etc.) \n",
    "    \n",
    "    The Shannon entropy H(X) for a random variable X with possible outcomes x_1, x_2, ... x_n and corresponding probabilities p(x_1), p(x_2), ... p(x_n)  is defined as:\n",
    "\n",
    "$$ H(X) = - \\sum_{i=1}^{n} p(x_i) \\log_2 p(x_i) $$\n",
    "\n",
    "- #### Minimum Entropy\n",
    "The minimum value of entropy is 0. This occurs when the system has no uncertainty, meaning there is only one possible outcome with a probability of 1.\n",
    "\n",
    "- #### Maximum Entropy\n",
    "The maximum value of entropy occurs when all outcomes are equally likely, meaning the probability distribution is uniform. For a system with 'n' possible outcomes, each with a probability of 1/n, the entropy is maximized. Mathematically, for p(x_i) = 1/n for all i.\n",
    "\n",
    "$$ H(X) = - \\sum_{i=1}^{n} {1/n}\\log_2 1/n = \\log_2 n $$\n",
    "\n",
    "\n",
    "#### Gini Impurity\n",
    "    Gini impurity is a measure used in decision tree algorithms to assess the quality of a split. It quantifies the likelihood of incorrect classification of a randomly chosen element if it were labeled according to the distribution of labels in the subset.\n",
    "    \n",
    "    The formula for Gini impurity G(X)G(X) for a dataset XX with possible classes c1,c2,…,cm and corresponding probabilities p(c1),p(c2),…,p(cm) is given by:\n",
    "\n",
    "$$ G(X) = 1−\\sum_{i=1}^m p(c_i)^2 $$\n",
    "\n",
    "- #### Minimum Gini Impurity\n",
    "The minimum value of Gini impurity is 0. This occurs when the dataset contains only one class (i.e., the distribution is perfectly pure). In this case, there is no uncertainty about the class of any randomly chosen element, and the Gini impurity is zero.\n",
    "\n",
    "- #### Maximum Gini Impurity\n",
    "The maximum value of Gini impurity occurs when the classes are equally distributed. For a binary classification (two classes), the maximum Gini impurity is 0.5. For n classes, it is 1-1/n.\n",
    "\n",
    "\n",
    "#### Information gain\n",
    "Information gain is a metric used to quantify the reduction in entropy or uncertainty about a random variable given additional information. It's commonly used in decision tree algorithms to decide which feature to split on at each step. Information gain is calculated as the difference between the entropy of the original dataset and the weighted sum of the entropies of the subsets after splitting on a particular feature.\n",
    "\n",
    "Steps to calculate information gain:\n",
    "\n",
    "   - Calculate the entropy of the dataset.\n",
    "   - Calculate the entropy of each subset resulting from the split.\n",
    "   - Compute the weighted sum of the entropies of the subsets.\n",
    "   - Calculate the information gain as the difference between the original entropy and the weighted sum of the entropies of the subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf7e227a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c    4\n",
      "b    3\n",
      "a    2\n",
      "Name: count, dtype: int64\n",
      "Entropy of a1b2c3d4: 1.5304930567574826\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_entropy(data: list):\n",
    "    # Count the frequency of each item in the data\n",
    "    counter = Counter(data)\n",
    "    \n",
    "    # Calculate the probabilities\n",
    "    total_count = sum(counter.values())\n",
    "    probabilities = [count / total_count for count in counter.values()]\n",
    "    \n",
    "    # Calculate the entropy\n",
    "    entropy = -sum(p * math.log2(p) for p in probabilities)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "# Example usage:\n",
    "a1b2c3d4 = ['a', 'a', 'b', 'b', 'b', 'c', 'c', 'c', 'c']\n",
    "a5d5 = ['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b']\n",
    "\n",
    "\n",
    "print(pd.Series(a1b2c3d4).value_counts())\n",
    "entropy = calculate_entropy(a1b2c3d4)\n",
    "print(f\"Entropy of a1b2c3d4: {entropy}\")\n",
    "\n",
    "# print(pd.Series(a5d5).value_counts())\n",
    "# entropy = calculate_entropy(a5d5)\n",
    "# print(f\"Entropy of a1b2c3d4: {entropy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5568eff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini Impurity: 0.6419753086419753\n"
     ]
    }
   ],
   "source": [
    "def calculate_gini_impurity(data):\n",
    "    # Count the frequency of each class in the data\n",
    "    counter = Counter(data)\n",
    "    \n",
    "    # Calculate the probabilities\n",
    "    total_count = sum(counter.values())\n",
    "    probabilities = [count / total_count for count in counter.values()]\n",
    "    \n",
    "    # Calculate the Gini impurity\n",
    "    gini_impurity = 1 - sum(p ** 2 for p in probabilities)\n",
    "    \n",
    "    return gini_impurity\n",
    "\n",
    "# Example usage:\n",
    "data = ['a', 'a', 'b', 'b', 'b', 'c', 'c', 'c', 'c']\n",
    "gini_impurity = calculate_gini_impurity(data)\n",
    "print(f\"Gini Impurity: {gini_impurity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15d967ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_information_gain(data: pd.DataFrame, feature:str, target:str):\n",
    "    # Calculate the entropy of the original dataset\n",
    "    print(data[target])\n",
    "    total_entropy = calculate_entropy(data[target])\n",
    "    print(total_entropy)\n",
    "    # Calculate the values and counts for the split attribute\n",
    "    counter = Counter(data[feature])\n",
    "    total_count = sum(counter.values())\n",
    "    print(total_count)\n",
    "    \n",
    "    \n",
    "    weighted_entropy = 0\n",
    "    for value, count in counter.items():\n",
    "        subset = data[data[feature]==value][target]\n",
    "        print(subset)\n",
    "        subset_entropy = calculate_entropy(subset)\n",
    "        weighted_entropy += (count / total_count) * subset_entropy\n",
    "        \n",
    "        # Calculate the information gain\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "    \n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02e0a114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Yes\n",
      "1     No\n",
      "2    Yes\n",
      "3     No\n",
      "4    Yes\n",
      "Name: target, dtype: object\n",
      "0.9709505944546686\n",
      "5\n",
      "0    Yes\n",
      "2    Yes\n",
      "Name: target, dtype: object\n",
      "1     No\n",
      "3     No\n",
      "4    Yes\n",
      "Name: target, dtype: object\n",
      "Information Gain for splitting on 'feature2': 0.4199730940219749\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "data = [\n",
    "    {'feature1': 'A', 'feature2': 'X', 'target': 'Yes'},\n",
    "    {'feature1': 'A', 'feature2': 'Y', 'target': 'No'},\n",
    "    {'feature1': 'B', 'feature2': 'X', 'target': 'Yes'},\n",
    "    {'feature1': 'B', 'feature2': 'Y', 'target': 'No'},\n",
    "    {'feature1': 'B', 'feature2': 'Y', 'target': 'Yes'},\n",
    "]\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "split_attribute = 'feature2'\n",
    "target_attribute = 'target'\n",
    "info_gain = calculate_information_gain(data, split_attribute, target_attribute)\n",
    "print(f\"Information Gain for splitting on '{split_attribute}': {info_gain}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e574f97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, criterion='gini', max_depth=None):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "    \n",
    "    def _build_tree(self, X, y, depth):\n",
    "        # Stopping criteria\n",
    "        if len(np.unique(y)) == 1 or len(y) == 0 or (self.max_depth and depth >= self.max_depth):\n",
    "            return {'type': 'leaf', 'class': np.argmax(np.bincount(y))}\n",
    "        \n",
    "        # Calculate the best split\n",
    "        best_feature, best_value = self._best_split(X, y)\n",
    "        \n",
    "        if best_feature is None:\n",
    "            return {'type': 'leaf', 'class': np.argmax(np.bincount(y))}\n",
    "        \n",
    "        # Split the data\n",
    "        if isinstance(best_value, (int, float)):\n",
    "            left_indices = X[:, best_feature] <= best_value\n",
    "            right_indices = X[:, best_feature] > best_value\n",
    "        else:\n",
    "            left_indices = X[:, best_feature] == best_value\n",
    "            right_indices = X[:, best_feature] != best_value\n",
    "            \n",
    "        left_tree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_tree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "        \n",
    "        return {'type': 'node', 'feature': best_feature, 'value': best_value, 'left': left_tree, 'right': right_tree}\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        best_feature = None\n",
    "        best_value = None\n",
    "        best_criterion_value = float('inf')\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        for feature in range(n_features):\n",
    "            unique_values = np.unique(X[:, feature])\n",
    "            for value in unique_values:\n",
    "                if isinstance(value, (int, float)):\n",
    "                    left_indices = X[:, feature] <= value\n",
    "                    right_indices = X[:, feature] > value\n",
    "                else:\n",
    "                    left_indices = X[:, feature] == value\n",
    "                    right_indices = X[:, feature] != value\n",
    "                    \n",
    "                if len(y[left_indices]) == 0 or len(y[right_indices]) == 0:\n",
    "                    continue\n",
    "                \n",
    "                criterion_value = self._criterion(y[left_indices], y[right_indices])\n",
    "                \n",
    "                if criterion_value < best_criterion_value:\n",
    "                    best_criterion_value = criterion_value\n",
    "                    best_feature = feature\n",
    "                    best_value = value\n",
    "        \n",
    "        return best_feature, best_value\n",
    "\n",
    "    def _criterion(self, left_y, right_y):\n",
    "        if self.criterion == 'gini':\n",
    "            return self._gini_impurity(left_y, right_y)\n",
    "        elif self.criterion == 'entropy':\n",
    "            return self._information_gain(left_y, right_y)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported criterion. Use 'gini' or 'entropy'.\")\n",
    "    \n",
    "    def _gini_impurity(self, left_y, right_y):\n",
    "        n = len(left_y) + len(right_y)\n",
    "        left_impurity = 1.0 - sum((np.sum(left_y == c) / len(left_y)) ** 2 for c in np.unique(left_y))\n",
    "        right_impurity = 1.0 - sum((np.sum(right_y == c) / len(right_y)) ** 2 for c in np.unique(right_y))\n",
    "        return (len(left_y) / n) * left_impurity + (len(right_y) / n) * right_impurity\n",
    "    \n",
    "    def _information_gain(self, left_y, right_y):\n",
    "        n = len(left_y) + len(right_y)\n",
    "        entropy_before = self._entropy(np.concatenate([left_y, right_y]))\n",
    "        entropy_after = (len(left_y) / n) * self._entropy(left_y) + (len(right_y) / n) * self._entropy(right_y)\n",
    "        return entropy_before - entropy_after\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        probabilities = [np.sum(y == c) / len(y) for c in np.unique(y)]\n",
    "        return -sum(p * np.log2(p) for p in probabilities)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return [self._predict(inputs, self.tree) for inputs in X]\n",
    "    \n",
    "    def _predict(self, inputs, tree):\n",
    "        if tree['type'] == 'leaf':\n",
    "            return tree['class']\n",
    "        \n",
    "        feature_value = inputs[tree['feature']]\n",
    "        if isinstance(tree['value'], (int, float)):\n",
    "            branch = tree['left'] if feature_value <= tree['value'] else tree['right']\n",
    "        else:\n",
    "            branch = tree['left'] if feature_value == tree['value'] else tree['right']\n",
    "        \n",
    "        return self._predict(inputs, branch)\n",
    "\n",
    "# Example usage:\n",
    "X = np.array([\n",
    "    ['A', 3], \n",
    "    ['A', 1], \n",
    "    ['B', 1], \n",
    "    ['B', 2], \n",
    "    ['B', 3], \n",
    "    ['B', 1]\n",
    "])\n",
    "y = np.array([0, 0, 1, 1, 1, 0])\n",
    "\n",
    "tree = DecisionTree(criterion='entropy')\n",
    "tree.fit(X, y)\n",
    "predictions = tree.predict(X)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a9eac13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'node',\n",
       " 'feature': 0,\n",
       " 'value': 'A',\n",
       " 'left': {'type': 'leaf', 'class': 0},\n",
       " 'right': {'type': 'node',\n",
       "  'feature': 1,\n",
       "  'value': '1',\n",
       "  'left': {'type': 'leaf', 'class': 0},\n",
       "  'right': {'type': 'leaf', 'class': 1}}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.tree"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
